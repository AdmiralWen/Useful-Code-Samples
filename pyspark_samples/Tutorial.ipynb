{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Operations in PySpark\n",
    "Notebook to demonstrate the basic operations in Pyspark. Most importantly, demonstrates parallelization across multiple nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pyspark libraries:\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define spark session:\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic RDD Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(firstname='James', middlename='', lastname='Smith', dob=datetime.date(1991, 4, 1), gender='M', salary=3000),\n",
       " Row(firstname='Michael', middlename='Rose', lastname='', dob=datetime.date(2000, 5, 19), gender='M', salary=4000),\n",
       " Row(firstname='Robert', middlename='', lastname='Williams', dob=datetime.date(1978, 9, 5), gender='M', salary=4000),\n",
       " Row(firstname='Maria', middlename='Anne', lastname='Jones', dob=datetime.date(1967, 12, 1), gender='F', salary=4000),\n",
       " Row(firstname='Jen', middlename='Mary', lastname='Brown', dob=datetime.date(1980, 2, 17), gender='F', salary=-1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a basic dataframe with column names:\n",
    "data = [('James','','Smith',date(1991,4,1),'M',3000),\n",
    "        ('Michael','Rose','',date(2000,5,19),'M',4000),\n",
    "        ('Robert','','Williams',date(1978,9,5),'M',4000),\n",
    "        ('Maria','Anne','Jones',date(1967,12,1),'F',4000),\n",
    "        ('Jen','Mary','Brown',date(1980,2,17),'F',-1)\n",
    "]\n",
    "\n",
    "columns = [\"firstname\", \"middlename\", \"lastname\", \"dob\", \"gender\", \"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(firstname='James', middlename='', lastname='Smith', dob=datetime.date(1991, 4, 1), gender='M', salary=3000),\n",
       " Row(firstname='Michael', middlename='Rose', lastname='', dob=datetime.date(2000, 5, 19), gender='M', salary=4000),\n",
       " Row(firstname='Robert', middlename='', lastname='Williams', dob=datetime.date(1978, 9, 5), gender='M', salary=4000),\n",
       " Row(firstname='Maria', middlename='Anne', lastname='Jones', dob=datetime.date(1967, 12, 1), gender='F', salary=4000),\n",
       " Row(firstname='Jen', middlename='Mary', lastname='Brown', dob=datetime.date(1980, 2, 17), gender='F', salary=-1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a basic dataframe with defined schema:\n",
    "data_schema = StructType([\n",
    "    StructField('firstname', StringType(), False),\n",
    "    StructField('middlename', StringType(), False),\n",
    "    StructField('lastname', StringType(), False),\n",
    "    StructField('dob', DateType(), False),\n",
    "    StructField('gender', StringType(), False),\n",
    "    StructField('salary', IntegerType(), False)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=data_schema)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brandonwen\\Anaconda3\\envs\\dbconnect\\lib\\site-packages\\pyspark\\sql\\context.py:117: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Reading in larger Dataframe (from Databricks default location):\n",
    "sqlContext = SQLContext(spark.sparkContext)\n",
    "dataPath = \"/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\"\n",
    "diamonds = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"inferSchema\", \"true\").load(dataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---------+-----+-------+-----+-----+-----+----+----+----+\n",
      "|_c0|carat|      cut|color|clarity|depth|table|price|   x|   y|   z|\n",
      "+---+-----+---------+-----+-------+-----+-----+-----+----+----+----+\n",
      "|  1| 0.23|    Ideal|    E|    SI2| 61.5| 55.0|  326|3.95|3.98|2.43|\n",
      "|  2| 0.21|  Premium|    E|    SI1| 59.8| 61.0|  326|3.89|3.84|2.31|\n",
      "|  3| 0.23|     Good|    E|    VS1| 56.9| 65.0|  327|4.05|4.07|2.31|\n",
      "|  4| 0.29|  Premium|    I|    VS2| 62.4| 58.0|  334| 4.2|4.23|2.63|\n",
      "|  5| 0.31|     Good|    J|    SI2| 63.3| 58.0|  335|4.34|4.35|2.75|\n",
      "|  6| 0.24|Very Good|    J|   VVS2| 62.8| 57.0|  336|3.94|3.96|2.48|\n",
      "|  7| 0.24|Very Good|    I|   VVS1| 62.3| 57.0|  336|3.95|3.98|2.47|\n",
      "|  8| 0.26|Very Good|    H|    SI1| 61.9| 55.0|  337|4.07|4.11|2.53|\n",
      "|  9| 0.22|     Fair|    E|    VS2| 65.1| 61.0|  337|3.87|3.78|2.49|\n",
      "| 10| 0.23|Very Good|    H|    VS1| 59.4| 61.0|  338| 4.0|4.05|2.39|\n",
      "| 11|  0.3|     Good|    J|    SI1| 64.0| 55.0|  339|4.25|4.28|2.73|\n",
      "| 12| 0.23|    Ideal|    J|    VS1| 62.8| 56.0|  340|3.93| 3.9|2.46|\n",
      "| 13| 0.22|  Premium|    F|    SI1| 60.4| 61.0|  342|3.88|3.84|2.33|\n",
      "| 14| 0.31|    Ideal|    J|    SI2| 62.2| 54.0|  344|4.35|4.37|2.71|\n",
      "| 15|  0.2|  Premium|    E|    SI2| 60.2| 62.0|  345|3.79|3.75|2.27|\n",
      "| 16| 0.32|  Premium|    E|     I1| 60.9| 58.0|  345|4.38|4.42|2.68|\n",
      "| 17|  0.3|    Ideal|    I|    SI2| 62.0| 54.0|  348|4.31|4.34|2.68|\n",
      "| 18|  0.3|     Good|    J|    SI1| 63.4| 54.0|  351|4.23|4.29| 2.7|\n",
      "| 19|  0.3|     Good|    J|    SI1| 63.8| 56.0|  351|4.23|4.26|2.71|\n",
      "| 20|  0.3|Very Good|    J|    SI1| 62.7| 59.0|  351|4.21|4.27|2.66|\n",
      "+---+-----+---------+-----+-------+-----+-----+-----+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use df.show() to get a tabular view:\n",
    "diamonds.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53940"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of rows:\n",
    "diamonds.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of distinct elements:\n",
    "diamonds.select('color').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+------------------+\n",
      "|color|clarity_count|         avg_price|\n",
      "+-----+-------------+------------------+\n",
      "|    J|            8|  5323.81801994302|\n",
      "|    I|            8| 5091.874953891553|\n",
      "|    H|            8| 4486.669195568401|\n",
      "|    G|            8| 3999.135671271697|\n",
      "|    F|            8| 3724.886396981765|\n",
      "|    E|            8|3076.7524752475247|\n",
      "|    D|            8|3169.9540959409596|\n",
      "+-----+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Groupby command:\n",
    "diamonds.groupBy('color').agg(countDistinct('clarity').alias('clarity_count'), mean('price').alias('avg_price')).orderBy(desc('color')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+-----+-------+-----+-----+-----+----+----+----+----------+\n",
      "|  _c0|carat|      cut|color|clarity|depth|table|price|   x|   y|   z|dense_rank|\n",
      "+-----+-----+---------+-----+-------+-----+-----+-----+----+----+----+----------+\n",
      "|27677| 2.19|    Ideal|    D|    SI2| 61.8| 57.0|18693|8.23|8.49|5.17|         1|\n",
      "|27668| 2.01|    Ideal|    D|    SI2| 62.1| 56.0|18674|8.02|8.11|5.01|         2|\n",
      "|27648| 2.11|  Premium|    D|    SI2| 60.9| 60.0|18575|8.28|8.21|5.02|         3|\n",
      "|27636| 1.04|Very Good|    D|     IF| 61.3| 56.0|18542|6.53|6.55|4.01|         4|\n",
      "|27628| 2.14|Very Good|    D|    SI2| 60.3| 60.0|18526|8.31|8.43|5.05|         5|\n",
      "|27721| 2.02|Very Good|    E|    SI1| 59.8| 59.0|18731|8.11| 8.2|4.88|         1|\n",
      "|27689| 1.51|    Ideal|    E|    VS1| 61.5| 57.0|18729|7.34| 7.4|4.53|         2|\n",
      "|27684|  2.0|Very Good|    E|    SI1| 60.5| 59.0|18709|8.09|8.14|4.94|         3|\n",
      "|27678| 1.28|    Ideal|    E|     IF| 60.7| 57.0|18700|7.09|6.99|4.27|         4|\n",
      "|27638| 1.72|Very Good|    E|    VS2| 63.4| 56.0|18557|7.65|7.55|4.82|         5|\n",
      "|27741| 1.71|  Premium|    F|    VS2| 62.3| 59.0|18791|7.57|7.53| 4.7|         1|\n",
      "|27738| 2.05|  Premium|    F|    SI2| 60.2| 59.0|18784|8.28|8.33| 5.0|         2|\n",
      "|27736|  1.6|    Ideal|    F|    VS1| 62.0| 56.0|18780|7.47|7.52|4.65|         3|\n",
      "|27734| 1.51|Very Good|    F|   VVS1| 62.6| 59.0|18777|7.33|7.24|4.56|         4|\n",
      "|27730|  2.0|Very Good|    F|    SI1| 57.9| 60.0|18759|8.28|8.34|4.81|         5|\n",
      "|27749|  2.0|Very Good|    G|    SI1| 63.5| 56.0|18818| 7.9|7.97|5.04|         1|\n",
      "|27748| 1.51|    Ideal|    G|     IF| 61.7| 55.0|18806|7.37|7.41|4.56|         2|\n",
      "|27747| 2.07|    Ideal|    G|    SI2| 62.5| 55.0|18804| 8.2|8.13|5.11|         3|\n",
      "|27742| 2.15|    Ideal|    G|    SI2| 62.6| 54.0|18791|8.29|8.35|5.21|         4|\n",
      "|27740|  2.8|     Good|    G|    SI2| 63.8| 58.0|18788| 8.9|8.85| 0.0|         5|\n",
      "+-----+-----+---------+-----+-------+-----+-----+-----+----+----+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using a Window function:\n",
    "from pyspark.sql.window import Window\n",
    "wnd = Window.partitionBy(\"color\").orderBy(desc(\"price\"))\n",
    "\n",
    "diamonds.withColumn(\"dense_rank\", dense_rank().over(wnd)).select(\"*\").filter(\"dense_rank <= 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|       dob|date_format|\n",
      "+----------+-----------+\n",
      "|1991-04-01| 04/01/1991|\n",
      "|2000-05-19| 05/19/2000|\n",
      "|1978-09-05| 09/05/1978|\n",
      "|1967-12-01| 12/01/1967|\n",
      "|1980-02-17| 02/17/1980|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reformating dates:\n",
    "df.select(col(\"dob\"), date_format(col(\"dob\"), \"MM/dd/yyyy\").alias(\"date_format\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+\n",
      "|       dob|     today|datediff|\n",
      "+----------+----------+--------+\n",
      "|1991-04-01|2023-04-21|   11708|\n",
      "|2000-05-19|2023-04-21|    8372|\n",
      "|1978-09-05|2023-04-21|   16299|\n",
      "|1967-12-01|2023-04-21|   20230|\n",
      "|1980-02-17|2023-04-21|   15769|\n",
      "+----------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Date diff:\n",
    "df.select(col(\"dob\"), current_date().alias(\"today\"), datediff(current_date(), col(\"dob\")).alias(\"datediff\")).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelization using .applyInPandas()\n",
    "It is possible to use `applyInPandas()` to perform some parallelization tasks that involve data processing. However this is usually not as straightforward as doing a `map()` or `mapPartitions()` operation over a RDD, as is shown in the next example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF partitions: 8\n"
     ]
    }
   ],
   "source": [
    "# First, repartition along the dimention to apply:\n",
    "diamonds = diamonds.repartition(8, 'clarity')\n",
    "print('DF partitions:', diamonds.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+\n",
      "|clarity|mean_diamond_dimensions|\n",
      "+-------+-----------------------+\n",
      "|   VVS2|               4.557346|\n",
      "|     I1|              5.8927937|\n",
      "|     IF|               4.339963|\n",
      "|    VS1|              4.8650045|\n",
      "|   VVS1|              4.3322444|\n",
      "|    VS2|              4.9360156|\n",
      "|    SI1|               5.138828|\n",
      "|    SI2|               5.582558|\n",
      "+-------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of apply function:\n",
    "def mean_diamond_dims(single_part_df):\n",
    "    ''' Function takes a partition of the input DF (partitioned by groupby()) as input, returns a Pandas DF as output. '''\n",
    "    # Groupby value:\n",
    "    clarity = single_part_df['clarity'].values[0]\n",
    "    # Column of means:\n",
    "    mean_xyz = (single_part_df['x'] + single_part_df['y'] + single_part_df['z'])/3\n",
    "    # Mean of means:\n",
    "    mean_all = np.mean(mean_xyz)\n",
    "    # Output:\n",
    "    return pd.DataFrame([{'clarity':clarity, 'mean_diamond_dimensions':mean_all}])\n",
    "\n",
    "# Be sure to define the schema of the pandas output correctly:\n",
    "out_schema = StructType([\n",
    "    StructField('clarity', StringType()),\n",
    "    StructField('mean_diamond_dimensions', FloatType())\n",
    "])\n",
    "\n",
    "diamonds.groupby('clarity').applyInPandas(mean_diamond_dims, schema = out_schema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|Output_Val|\n",
      "+----------+\n",
      "|0.35957536|\n",
      "|0.35957536|\n",
      "|0.50110745|\n",
      "|0.72847253|\n",
      "|0.50110745|\n",
      "|0.52407885|\n",
      "| 0.7612426|\n",
      "|0.66044647|\n",
      "+----------+\n",
      "\n",
      "Total runtime: 52.50363731384277\n"
     ]
    }
   ],
   "source": [
    "# Parallelizing some process:\n",
    "def some_complex_func(partial_df):\n",
    "    # Simulating some long-running function or process:\n",
    "    time.sleep(10)\n",
    "    return pd.DataFrame([{'Output_Val':np.random.uniform()}])\n",
    "\n",
    "# Output schema:\n",
    "out_schema = StructType([\n",
    "    StructField('Output_Val', FloatType())\n",
    "])\n",
    "\n",
    "t0 = time.time()\n",
    "diamonds.groupby('clarity').applyInPandas(some_complex_func, schema = out_schema).show()\n",
    "print('Total runtime:', time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelization using map() and mapPartitions()\n",
    "The `map()` and `mapPartitions()` operations are the basic workhorses of parallelization in Spark. Very often, this is the better choice when compared with `applyInPandas()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define spark session and context:\n",
    "spark = SparkSession.builder.appName('loan_vectors').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# Check the number of executors available in our spark session:\n",
    "print(sc._jsc.sc().getExecutorMemoryStatus().size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16]\n"
     ]
    }
   ],
   "source": [
    "# Example of a basic RDD map(), which applies a function over each element of the RDD:\n",
    "rdd = sc.parallelize([1, 2, 3, 4])\n",
    "rdd_calc = rdd.map(lambda x: x**2)\n",
    "print(rdd_calc.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 5, 14], [4, 5, 7, 16], [6, 7, 9, 18], [8, 9, 11, 20],\n",
      " [10, 11, 13, 22], [12, 13, 15, 24], [14, 15, 17, 26], [16, 17, 19, 28]]\n",
      "Runtime: 9.748255252838135\n"
     ]
    }
   ],
   "source": [
    "# A more advanced example using map(), this time the RDD is a partition of input lists:\n",
    "\n",
    "# Calculation functions for parallelizing:\n",
    "def add_func(x, y):\n",
    "    time.sleep(2)\n",
    "    return x+y\n",
    "\n",
    "def spark_run(input_list):\n",
    "    out_list = [add_func(i[0], i[1]) for i in input_list]\n",
    "    return out_list\n",
    "\n",
    "# Define input list\n",
    "list1 = [(1, 1), (2, 1), (3, 2), (4, 10)]\n",
    "\n",
    "# Start of processing:\n",
    "t0 = time.time()\n",
    "workers = 8\n",
    "partitions = [[(p[0]+i, p[1]+i) for p in list1] for i in range(workers)]\n",
    "rdd = sc.parallelize(partitions)\n",
    "rdd_out = rdd.map(lambda x: spark_run(x))\n",
    "print(rdd_out.collect())\n",
    "print('Runtime:', time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 7]\n"
     ]
    }
   ],
   "source": [
    "# Unlike map(), mapPartitions() applies a function to each partition of a RDD rather than each element:\n",
    "def sum_partition(iterator):\n",
    "    yield sum(iterator)\n",
    "\n",
    "rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
    "sum_rdd = rdd.mapPartitions(sum_partition)\n",
    "print(sum_rdd.collect())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
